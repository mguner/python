{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Introduction to Matrix Algebra](#matrix_algebra)\n",
    "2. [The Least Squares Method for Data Science](#the_least_squares_for_data_science)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"matrix_algebra\">\n",
    "</a>\n",
    "\n",
    "<center> <h1>Introduction to Matrix Algebra</h1> </center>\n",
    "\n",
    " To be able to understand the least squares method from matrix algebra perspective we should recall some of the linear algebra definitions first. \n",
    " \n",
    "### The dot product of two vectors in $\\mathbb{R}^n$\n",
    " \n",
    " Let us consider two vectors $\\vec{u}$ and $\\vec{v}$ in $\\mathbb{R}^n$. In coordinates we could write write these vectors as  $\\vec{u} = (u_1, \\cdots u_{n})$ and $ \\vec{v} = (v_1 \\cdots v_{n})$ . Recall that the dot product of $\\vec{u}$ and $\\vec{v}$ is given in two different but equal ways mathematically. \n",
    " \n",
    " $$ \\vec{u} \\cdot \\vec{v} = \\sum\\limits_{i=1}^{n} u_i   v_i = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n$$\n",
    " \n",
    " In particular if we are working in 3-dimension the expression above becomes:\n",
    " \n",
    "  $$ \\vec{u} \\cdot \\vec{v} =  u_1 v_1 + u_2 v_2 + u_3 v_3 $$\n",
    "\n",
    " \n",
    " \n",
    " One can show using some linear algebra that in $\\mathbb{R}^{n}$ this formula is equaivalent to the:\n",
    " \n",
    " $$ \\vec{u} \\cdot \\vec{v} = |u||v|\\cos(\\theta)$$ \n",
    " \n",
    " where $|u|$ represents the length of the vector $\\vec{u} \\in \\mathbb{R}^{n}$. Note that two different but equal versions of the dot product allow us to measure angles between vectors. For example if $\\vec{u}$ is perpendicular to $\\vec{v}$ then $\\cos(\\theta) = 0$ so $ \\vec{u}\\cdot \\vec{v} = 0$. Also very clearly, we can see that if the dot product of two non-zero vectors is zero then the angle between them should be $\\frac{\\pi}{2}$. Finally note that $\\vec{u}\\cdot \\vec{u} = |u|^2$\n",
    " \n",
    " \n",
    " For more on the inner products:\n",
    " \n",
    " [Andrew Ng - Machine Learning](https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72)\n",
    " \n",
    " [Khan Academy](https://www.youtube.com/watch?v=WNuIhXo39_k)\n",
    " ### Left Multiplication of a Matrix with a column vector. \n",
    " \n",
    " let us consider a matrix $A = \\big[A_{ij}\\big]_{n\\times n}$ and a vector $\\vec{x} = (x_1 \\cdots x_n)$. Then the multiplication $A \\vec{x} = b $ can be written as $b_i  = \\sum \\limits_{j =1}^{n} A_{ij}x_j $. Let's make it more concrete. Suppose we are working with $3\\times 3 $ matrix $A$. Then \n",
    " \n",
    " $$\\vec{b} =\\begin{bmatrix}\n",
    "    A_{11} & A_{12} & A_{13} \\\\\n",
    "    A_{12} & A_{22} & A_{23} \\\\\n",
    "    A_{13} & A_{23} & A_{33}\\\\\n",
    "\\end{bmatrix}   \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "    \\vec{r_1} \\cdot \\vec{x} \\\\\n",
    "    \\vec{r_2} \\cdot \\vec{x} \\\\\n",
    "    \\vec{r_3} \\cdot \\vec{x}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\\n",
    "    A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\\\\n",
    "    A_{31}x_1 + A_{32}x_2 + A_{33}x_3 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "\n",
    "where $\\vec{r_i}$ repsents $i^{th}$-row of the matrix $A$. We will refer this interpretation of matrix multiplication as *'dot_product-version'*. Now the same equation can be seen as the linear combination of the columns of the matrix $A$:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    A_{11} \\\\\n",
    "    A_{21} \\\\\n",
    "    A_{31} \n",
    "\\end{bmatrix} x_1 + \\begin{bmatrix}\n",
    "    A_{12} \\\\\n",
    "    A_{22} \\\\\n",
    "    A_{32} \n",
    "\\end{bmatrix} x_2 +  \\begin{bmatrix}\n",
    "    A_{13} \\\\\n",
    "    A_{23} \\\\\n",
    "    A_{33} \n",
    "\\end{bmatrix} x_3  =\\begin{bmatrix}\n",
    "    A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\\n",
    "    A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\\\\n",
    "    A_{31}x_1 + A_{32}x_2 + A_{33}x_3 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "Due to the second interpretation we can say if $$A \\vec{x} = \\vec{b}$$ then $\\vec{b}$ lies on the column space of A as it can be expressed as a linear combination of the column vectors of A. We will refer this interpretation of the matrix multiplication as *'linear combinations of columns'*.\n",
    "\n",
    "- Review for matrix multiplication\n",
    "\n",
    "[G. Strang- The Column Space of a matrix](https://www.youtube.com/watch?v=n98ilenWoak)\n",
    "\n",
    "[Dot product interpretation of matrix multiplication](https://www.youtube.com/watch?v=Awcj447pYuk)\n",
    "\n",
    "[Matrix multiplication - both interpretation](https://www.youtube.com/watch?v=7Mo4S2wyMg4)\n",
    "\n",
    "\n",
    "<a name=\"the_least_squares_for_data_science\">\n",
    "</a>\n",
    "\n",
    "<center> <h1>The Least Square Method for Data Science</h1> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a least squares figure\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# %matplotlib inline\n",
    "\n",
    "# noise = np.random.normal(0, 4, 11)\n",
    "# x_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "# x = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "# line = [3*i +4 for i in x_list ]\n",
    "# y_list = [int(i+j) for (i,j) in zip(line, noise)]\n",
    "# y = pd.DataFrame([i+j for (i,j) in zip(line, noise)])\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# ax.scatter(x, line, c = 'r')\n",
    "# ax.plot(x,line, c = 'black', alpha = 0.7, linewidth = 3)\n",
    "# # Sample data to play with.\n",
    "# ax.scatter(x, y)\n",
    "\n",
    "# for i in range(len(noise)):\n",
    "#     if y_list[i] <= line[i]:\n",
    "#         ax.vlines(x = x_list[i], ymin = y_list[i], ymax = y_list[i] - noise[i], colors= 'g')\n",
    "#     else:\n",
    "#         ax.vlines(x = x_list[i], ymin = line[i], ymax = line[i] + noise[i], colors = 'g')\n",
    "\n",
    "        \n",
    "# plt.savefig('leastsquare.png');\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: Fitting the 'best line'\n",
    "For the least square method we say 'the best line' should make the errors (green line segments) as small as possible. Let's see what this corresponds mathematically. \n",
    "\n",
    "\n",
    "Suppose we are given n-sample in the form $$\\{(x_1, y_1), (x_2, y_2), (x_3, y_3), \\cdots, (x_n, y_n)\\}$$ \n",
    "\n",
    "Recall that in basic linear regression, we are trying to find a line so that line passes from  each of the points $(x_i, y_i)$. Mathematically, this is equivalent to find $C, D$ so that $$ C +  D x_i = y_i $$ for all $i \\in \\{1, \\cdots n\\}$. In fact, if we write this more explicitly we would see something like this:\n",
    "\n",
    "$$ \\begin{eqnarray}C + D x_1 &=& y_1 \\\\\n",
    "C + D x_2 &=& y_2 \\\\\n",
    "C + D x_3 & =& y_3 \\\\\n",
    "C+ D x_4 & = & y_4 \\\\\n",
    "\\vdots \\\\\n",
    "C + D x_n &= & y_n\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "\n",
    "Note that matrix algebra is perfect to write this kind of system of equations in a compact way. So  in linear algebra language the system of equations can be written as simple as: line as $A \\vec{u} = \\vec{y} $. More explicitly:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix}\n",
    "    1 & x_{1} \\\\\n",
    "    1 & x_{2}  \\\\\n",
    "    1 & x_{3} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "    1 & x_{n}\n",
    "\\end{bmatrix}}_A  \\underbrace{\\begin{bmatrix}\n",
    "    C \\\\\n",
    "    D  \n",
    "\\end{bmatrix}}_{\\vec{u}} = \\underbrace{\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    y_3 \\\\\n",
    "    \\vdots\\\\\n",
    "    y_n\n",
    "\\end{bmatrix}}_{\\vec{y}}$$ \n",
    "\n",
    "#### Can we always solve a system of linear equations?\n",
    "\n",
    "First we should ask ourselves whether such $C, D$ exists. This means can we write $\\vec{y}$ as a linear combination of columns vectors of $A$. Notice that this problem is equivalent to ask whether or not $\\vec{y}$ in the column space of A. Also solving this problem is relatively straight forward. \n",
    "\n",
    "[Solving system of linear equations - MIT](https://www.youtube.com/watch?v=xCIXkm3-ocQ)\n",
    "\n",
    "[Solving system of linear equations](https://www.youtube.com/watch?v=CsTOUbeMPUo)\n",
    "\n",
    "[Inconsistent system of linear equations](https://www.youtube.com/watch?v=oGwNLitgbqY)\n",
    "\n",
    "We know that in application because of the noise/errors in experiments etc.,  it is almost impossible to get a sample that perfectly lies on a line (even if the relation between x and y is actually linear). Therefore most of the time we know that there is no solution for the equation above. This means for any $C, D$ we will get $A\\vec{u} \\neq \\vec{y}$ and obviously for different values of $C, D$ we will get different errors.  Therefore for a particular value of $C, D$ we can find the error vector by $ \\vec{e} = A\\vec{u}  - \\vec{y}$. Note that $\\vec{e} = \\{e_1 \\cdots e_n \\}$ and basically the absolute value of $e_i$ correspons to the lenght of the $i^{th}$ green line segment below.\n",
    "\n",
    "![Leastsquare](leastsquare.png)\n",
    "\n",
    "\n",
    "#### What is the next best thing than solving a system of linear equations: Find projection of $\\vec{y}$ onto the column space of A.\n",
    "Then we enter to the next phase. The least squares method basically try to find a solution $(C,D)$ so that the length of the error vector is the minimum. Most of the time we see this problem from calculus perspective as finding the derivatives and set them equal to zero. However linear algebra offers a more geometric picture. \n",
    "\n",
    "Note that we already assume that the vector $\\vec{y}$ is not in the column space of A. Let's try to visualize this column space as a plane (containing zero as it is a subspace) in the space. Since $\\vec{y}$ is not in it we will show $\\vec{y}$ as another vector on the space.\n",
    "\n",
    "![projection](projection.png)\n",
    "\n",
    "\n",
    "Note that the error vector is denoted by $\\vec{z}$ on the above figure. But more importantly it is clear that the error vector would be minumum when it is perpendicular to the plane. This means the error vector should be perpendicular to every vector in the column space of A. We can write this condition as:\n",
    "\n",
    "$$ \\vec{C_i} \\cdot \\vec{e} = 0 $$\n",
    "\n",
    "\n",
    "Where $\\vec{C_i}$ is $i^{th}$ column vector of $A$:\n",
    "\n",
    "\n",
    "\n",
    "$$A =\\begin{bmatrix}\n",
    "    |   & \\dots & |   &  \\dots & |    \\\\\n",
    "    |   & \\dots & |   &  \\dots & |     \\\\\n",
    "    C_1 & \\dots & C_i &  \\dots & C_{n}  \\\\\n",
    "    |   & \\dots & |   &  \\dots & |       \\\\\n",
    "    |   & \\dots & |   &  \\dots & |   \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This condition can be easily written as $A^{T} \\vec{e} = 0$: The reason for this is the following. Recall that $A$-transpose is the matrix created by making columns rows and rows to column:\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix}\n",
    "    ----    &  C_{1}^{T} &  ----   \\\\\n",
    "    \\vdots    &  \\vdots    &  \\vdots    \\\\\n",
    "    ----    &  C_{i}^{T} &  ----      \\\\\n",
    "     \\vdots    &  \\vdots    &  \\vdots     \\\\\n",
    "     ----    &  C_{n}^{T} &  ----  \n",
    "\\end{bmatrix}}_{A^{T}} \\cdot \\underbrace{\\begin{bmatrix}\n",
    "    e_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    e_i \\\\\n",
    "    \\vdots\\\\\n",
    "    e_n\n",
    "\\end{bmatrix}}_{\\vec{e}} = \\begin{bmatrix}\n",
    "    C_{1} \\cdot \\vec{e}  \\\\\n",
    "    \\vdots \\\\\n",
    "    C_{i} \\cdot \\vec{e}\\\\\n",
    "    \\vdots\\\\\n",
    "   C_{n} \\cdot \\vec{e\n",
    "  }\n",
    "\\end{bmatrix} = 0$$\n",
    "\n",
    "\n",
    "We can finish this part by recalling $\\vec{e} = A\\vec{u} - \\vec{y}$. So the equation above become: \n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "A^{T}(A\\vec{u} - \\vec{y}) &=& 0 \\\\\n",
    "A^{T}A \\vec{u} - A^{T}\\vec{y} &=& 0 \\\\\n",
    "A^{T}A \\vec{u} & = & A^{T}\\vec{y} \\\\\n",
    "(A^{T}A)^{-1}A^{T}\\vec{y} &=& \\vec{u}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Note that we are given $A$ and $\\vec{y}$ so by applying above formula we get 'the best' solution $\\vec{u}$. However note that the projection of $\\vec{y}$ onto column space is given by $A\\vec{u} = A(A^{T}A)^{-1}A^{T}\\vec{y}$. This is why the matric on the left hand side $P = A(A^{T}A)^{-1}A^{T} $ is called the projection matrix of A.\n",
    "\n",
    "\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix}\n",
    "    ----    &  C_{1}^{T} &  ----   \\\\\n",
    "    \\vdots    &  \\vdots    &  \\vdots    \\\\\n",
    "    ----    &  C_{i}^{T} &  ----      \\\\\n",
    "     \\vdots    &  \\vdots    &  \\vdots     \\\\\n",
    "     ----    &  C_{n}^{T} &  ----  \n",
    "\\end{bmatrix}}_{A^{T}} \\cdot \\begin{bmatrix}\n",
    "    |   & \\dots & |   &  \\dots & |    \\\\\n",
    "    |   & \\dots & |   &  \\dots & |     \\\\\n",
    "    C_1 & \\dots & C_i &  \\dots & C_{n}  \\\\\n",
    "    |   & \\dots & |   &  \\dots & |       \\\\\n",
    "    |   & \\dots & |   &  \\dots & |   \n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
