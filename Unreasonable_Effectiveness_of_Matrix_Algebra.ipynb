{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. [Introduction to Matrix Algebra](#matrix_algebra)\n",
    "2. [The Least Squares Method for Data Science](#the_least_squares_for_data_science)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The least squares model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"matrix_algebra\">\n",
    "</a>\n",
    "\n",
    "<center> <h1>Introduction to Matrix Algebra</h1> </center>\n",
    "\n",
    " To be able to understand the least squares method with matrix algebra jargon then we should recall some of the notions first. \n",
    " \n",
    "### The dot product of two vectors in $\\mathbb{R}^n$\n",
    " \n",
    " Let us consider two vectors $\\vec{u}$ and $\\vec{v}$ in $\\mathbb{R}^n$. In coordinates we could write write these vectors as  $\\vec{u} = (u_1, \\cdots u_{n})$ and $ \\vec{v} = (v_1 \\cdots v_{n})$ . Recall that the dot product of $\\vec{u}$ and $\\vec{v}$ is given in two different but equal ways mathematically. \n",
    " \n",
    " $$ \\vec{u} \\cdot \\vec{v} = \\sum\\limits_{i=1}^{n} u_i   v_i = u_1 v_1 + u_2 v_2 + \\cdots u_n v_n$$\n",
    " \n",
    " \n",
    " One can show using some linear algebra that in $\\mathbb{R}^{n}$ this formula is equaivalent to the:\n",
    " \n",
    " $$ \\vec{u} \\cdot \\vec{v} = |u||v|\\cos(\\theta)$$ \n",
    " \n",
    " where $|u|$ represents the length of the vector $\\vec{u} \\in \\mathbb{R}^{n}$. Note that two different but equal versions of the dot product allow us to measure angles between vectors. For example if $\\vec{u}$ is perpendicular to $\\vec{v}$ then $\\cos(\\theta) = 0$ so $ \\vec{u}\\cdot \\vec{v} = 0$. Also very clearly, we can see that if the dot product of two non-zero vectors is zero then the angle between them should be $\\frac{\\pi}{2}$. Finally note that $\\vec{u}\\cdot \\vec{u} = |u|^2$\n",
    " \n",
    " \n",
    " For more on the inner products:\n",
    " \n",
    " [Andrew Ng - Machine Learning](https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72)\n",
    " \n",
    " [Khan Academy](https://www.youtube.com/watch?v=WNuIhXo39_k)\n",
    " ### Left Multiplication of a Matrix with a column vector. \n",
    " \n",
    " let us consider a matrix $A = \\big[A_{ij}\\big]_{n\\times n}$ and a vector $\\vec{u} = (u_1 \\cdots u_n)$. Then the multiplication $A \\vec{u} = b $ can be written as $b_i  = \\sum \\limits_{j =1}^{n} A_{ij}u_j $. Let's make it more concrete. Suppose we are working with $3\\times 3 $ matrix $A$. Then \n",
    " \n",
    " $$\\vec{b} =\\begin{bmatrix}\n",
    "    A_{11} & A_{12} & A_{13} \\\\\n",
    "    A_{12} & A_{22} & A_{23} \\\\\n",
    "    A_{13} & A_{23} & A_{33}\\\\\n",
    "\\end{bmatrix}   \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "    \\vec{r_1} \\cdot \\vec{u} \\\\\n",
    "    \\vec{r_2} \\cdot \\vec{u} \\\\\n",
    "    \\vec{r_3} \\cdot \\vec{u}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\\n",
    "    A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\\\\n",
    "    A_{31}x_1 + A_{32}x_2 + A_{33}x_3 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "\n",
    "where $\\vec{r_i}$ repsents $i^{th}$-row of the matrix $A$. We will refer this interpretation of matrix multiplication as *'dot_product-version'*. Now the same equation can be seen as the linear combination of the columns of the matrix $A$:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "    A_{11} \\\\\n",
    "    A_{21} \\\\\n",
    "    A_{31} \n",
    "\\end{bmatrix} x_1 + \\begin{bmatrix}\n",
    "    A_{12} \\\\\n",
    "    A_{22} \\\\\n",
    "    A_{32} \n",
    "\\end{bmatrix} x_2 +  \\begin{bmatrix}\n",
    "    A_{13} \\\\\n",
    "    A_{23} \\\\\n",
    "    A_{33} \n",
    "\\end{bmatrix} x_3  =\\begin{bmatrix}\n",
    "    A_{11}x_1 + A_{12}x_2 + A_{13}x_3 \\\\\n",
    "    A_{21}x_1 + A_{22}x_2 + A_{23}x_3 \\\\\n",
    "    A_{31}x_1 + A_{32}x_2 + A_{33}x_3 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    b_3\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "Due to the second interpretation we can say if $$A \\vec{x} = \\vec{b}$$ then $\\vec{b}$ lies on the column space of A as it can be expressed as a linear combination of the column vectors of A. We will refer this interpretation of the matrix multiplication as *'linear combinations of columns'*.\n",
    "\n",
    "[Dot product interpretation of matrix multiplication](https://www.youtube.com/watch?v=Awcj447pYuk)\n",
    "\n",
    "[Matrix multiplication - both interpretation](https://www.youtube.com/watch?v=7Mo4S2wyMg4)\n",
    "\n",
    "\n",
    "<a name=\"the_least_squares_for_data_science\">\n",
    "</a>\n",
    "\n",
    "<center> <h1>The Least Square Method for Data Science</h1> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "noise = np.random.normal(0, 3, 11)\n",
    "x_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "x = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "line = [3*i +4 for i in x_list ]\n",
    "y_list = [int(i+j) for (i,j) in zip(line, noise)]\n",
    "y = pd.DataFrame([i+j for (i,j) in zip(line, noise)])\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x, line, c = 'r')\n",
    "ax.plot(x,line, c = 'y', alpha = 0.5)\n",
    "# Sample data to play with.\n",
    "ax.scatter(x, y)\n",
    "\n",
    "for i in range(len(noise)):\n",
    "    if y_list[i] <= line[i]:\n",
    "        ax.vlines(x = x_list[i], ymin = y_list[i], ymax = y_list[i] - noise[i], colors= 'g')\n",
    "    else:\n",
    "        ax.vlines(x = x_list[i], ymin = line[i], ymax = line[i] + noise[i], colors = 'g')\n",
    "\n",
    "        \n",
    "plt.savefig('leastsquare.png')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our goal is to fit 'the best line'. For the least square method we say 'the best line' should make the errors (green line segments) as small as possible. Let's see what this corresponds mathematically. If we label each data as its coordinates $(x_i, y_i)$ then if we think an arbitrary line in the form $\\hat{y} = ax + b$ the errors can be written as $e_i  =\\hat{y}_i - y_i = (a x_{i} +b) - y_i$. Note that for each data (for each i) we will get a different equation and therefore we got a system of equations. This system can be written as $$\\vec{e} = [a]_{1x1}\\vec{x} -\\vec{y} + \\vec{b}$$\n",
    "\n",
    "\n",
    "Suppose we are given n-sample in the form $$\\{(x_1, y_1), (x_2, y_2), \\cdots (x_3, y_3), (x_n, y_n)\\}$$ \n",
    "\n",
    "Recall that in basic linear regression we are trying to find a line so that line passes from  each of the points $(x_i, y_i)$. Mathematically, this is equivalent to find $C, D$ so that $$ C +  D x_i = y_i $$ for all $i \\in \\{1, \\cdots n\\}$. In fact, if we write this more explicitly we would see something like this:\n",
    "\n",
    "$$ \\begin{eqnarray}C + D x_1 &=& y_1 \\\\\n",
    "C + D x_2 &=& y_2 \\\\\n",
    "C + D x_3 & =& y_3 \\\\\n",
    "C+ D x_4 & = & y_4 \\\\\n",
    "\\vdots \\\\\n",
    "C + D x_n &= & y_n\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "\n",
    "Note that matrix algebra is perfect to write this kind of system of equations in a compact way. So  in linear algebra language the system of equations can be written as simple as: line as $A \\vec{u} = \\vec{y} $. More explicitly:\n",
    "\n",
    "$$ \\underbrace{\\begin{bmatrix}\n",
    "    1 & x_{1} \\\\\n",
    "    1 & x_{2}  \\\\\n",
    "    1 & x_{3} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "    1 & x_{n}\n",
    "\\end{bmatrix}}_A  \\underbrace{\\begin{bmatrix}\n",
    "    C \\\\\n",
    "    D  \n",
    "\\end{bmatrix}}_{\\vec{u}} = \\underbrace{\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    y_3 \\\\\n",
    "    \\vdots\\\\\n",
    "    y_n\n",
    "\\end{bmatrix}}_{\\vec{y}}$$ \n",
    "\n",
    "First we should ask ourselves whether such $C, D$ exists. This means can we write $\\vec{y}$ as a linear combination of columns vectors of $A$. Notice that this problem is equivalent to ask whether or not $\\vec{y}$ in the column space of A. Also solving this problem is relatively straight forward. \n",
    "\n",
    "[Solving system of linear equations](https://www.youtube.com/watch?v=CsTOUbeMPUo)\n",
    "\n",
    "[Inconsistent system of linear equations](https://www.youtube.com/watch?v=oGwNLitgbqY)\n",
    "\n",
    "We know that in real life because of the noise it is almost impossible to get a sample that perfectly lies on a line. Therefore most of the time we know that there is no solution for the equation above. This means for any $C, D$ we will get $A\\vec{u} \\neq \\vec{y}$ and obviously for different values of $C, D$ we will get different errors.  Therefore for a particular value of $C, D$ we can find the error vector by $ \\vec{e} = A\\vec{u}  - \\vec{y}$. Note that $\\vec{e} = \\{e_1 \\cdots e_n \\}$ and basically the absolute value of $e_i$ correspons to the lenght of the $i^{th}$ green line segment below.\n",
    "\n",
    "![Leastsquare](leastsquare.png)\n",
    "\n",
    "\n",
    "\n",
    "Then we enter to the next phase. The least squares method basically try to find a solution $(C,D)$ so that the length of the error vector is the minimum. Most of the time we see this problem from calculus perspective as finding the derivatives and set them equal to zero. However linear algebra offers a more geometric picture. \n",
    "\n",
    "Note that we already assume that the vector $\\vec{y}$ is not in the column space of A. Let's try to visualize this column space as a plane (containing zero as it is a subspace) in the space. Since $\\vec{y}$ is not in it we will show $\\vec{y}$ as another vector on the space.\n",
    "\n",
    "![projection](projection.png)\n",
    "\n",
    "\n",
    "Note that the error vector is denoted by $\\vec{z}$ on the above figure. But more importantly it is clear that the error vector would be minumum when it is perpendicular to the plane. This means the error vector should be perpendicular to every vector in the column space of A. We can write this condition as:\n",
    "\n",
    "$$ \\vec{C_i} \\cdot \\vec{e} = 0 $$\n",
    "\n",
    "\n",
    "Where $\\vec{C_i}$ is $i^{th}$ column vector of $A$:\n",
    "\n",
    "\n",
    "\n",
    "$$A =\\begin{bmatrix}\n",
    "    |   & \\dots & |   &  \\dots & |    \\\\\n",
    "    |   & \\dots & |   &  \\dots & |     \\\\\n",
    "    C_1 & \\dots & C_i &  \\dots & C_{n}  \\\\\n",
    "    |   & \\dots & |   &  \\dots & |       \\\\\n",
    "    |   & \\dots & |   &  \\dots & |   \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "This condition can be easily written as $A^{T} \\vec{e} = 0$: The reason for this is the following. Recall that $A$-transpose is the matrix created by making columns rows and rows to column:\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix}\n",
    "    ----    &  C_{1}^{T} &  ----   \\\\\n",
    "    \\vdots    &  \\vdots    &  \\vdots    \\\\\n",
    "    ----    &  C_{i}^{T} &  ----      \\\\\n",
    "     \\vdots    &  \\vdots    &  \\vdots     \\\\\n",
    "     ----    &  C_{n}^{T} &  ----  \n",
    "\\end{bmatrix}}_{A^{T}} \\cdot \\underbrace{\\begin{bmatrix}\n",
    "    e_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    e_i \\\\\n",
    "    \\vdots\\\\\n",
    "    e_n\n",
    "\\end{bmatrix}}_{\\vec{e}} = \\begin{bmatrix}\n",
    "    C_{1} \\cdot e_1  \\\\\n",
    "    \\vdots \\\\\n",
    "    C_{i} \\cdot e_i\\\\\n",
    "    \\vdots\\\\\n",
    "   C_{n} \\cdot e_n\n",
    "\\end{bmatrix} = 0$$\n",
    "\n",
    "\n",
    "We can finish this part by recalling $\\vec{e} = A\\vec{u} - \\vec{y}$. So the equation above become: \n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{eqnarray}\n",
    "A^{T}(A\\vec{u} - \\vec{y}) &=& 0 \\\\\n",
    "A^{T}A \\vec{u} - A^{T}\\vec{y} &=& 0 \\\\\n",
    "A^{T}A \\vec{u} & = & A^{T}\\vec{y} \\\\\n",
    "(A^{T}A)^{-1}A^{T}\\vec{y} &=& \\vec{u}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Note that we are given $A$ and $\\vec{y}$ so by applying above formula we get 'the best' solution $\\vec{u}$. However note that the projection of $\\vec{y}$ onto column space is given by $A\\vec{u} = A(A^{T}A)^{-1}A^{T}\\vec{y}$. This is why the matric on the left hand side $P = A(A^{T}A)^{-1}A^{T} $ is called the projection matrix of A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
